import * as Queue from 'bull'
import chalk from 'chalk'

const scrapeAttributes = new Queue('scrape_attributes', process.env.REDIS_URL)
const store = new Queue('store_results', process.env.REDIS_URL)
const sites = require('./sites.json')

scrapeAttributes.process('scrape', 1, require('./jobs/scrape.ts').default)
scrapeAttributes
  .on('error', error => console.error('Error scraping', error))
  .on('active', job => console.log('\n\nü†Æ Scrape', job.data.name, '\n     üåê', job.data.url))
  .on('completed', function(job: Queue.Job, items: any[]) {
    console.log('\nFound', items.length, 'results for', job.data.name)

    for (const item of items) {
      store.add('store', item)
    }
  })

store
  .on('completed', (job: Queue.Job) => {
    console.log('    ', chalk.green('‚úì'), job.data.name)
  })
  .on('error', (error: Error, job: Queue.Job) => {
    console.log('    ', chalk.red('‚®Ø'), job.data.name)
  })

store.process('store', 500, require('./jobs/store'))

function getPaginatedURLs(url: string, pageQuery: string, pages: number) {
  const urlParts = url.split('?')
  const searchParams = new URLSearchParams(urlParts[1])
  const urls = []

  for (let i = 1; i < (pages + 1); i++) {
    searchParams.set(pageQuery, i.toString())
    const paginatedURL = new URL(urlParts[0])
    paginatedURL.search = searchParams.toString()
    urls.push(paginatedURL.toString())
  }

  return urls;
}

function scrape() {
  sites.forEach((siteData: any) => {
    const { type, url, pages } = siteData
    const contract: any = require(`./contracts/${type}.json`)
    const { pageQuery } = contract

    if (pageQuery === undefined && pages === undefined) {
      scrapeAttributes.add('scrape', { name: type, contract, url })
    } else {
      const urls = getPaginatedURLs(url, pageQuery, pages)
      urls.forEach((url: string) => {
        scrapeAttributes.add('scrape', { name: type, contract, url })
      })
    }
  })
}

function run() {
  console.log(chalk.bgGreen('üè† immo-feed \n\n'))

  scrape()

  setInterval(function() {
    scrape()
  }, parseInt(process.env.SCRAPER_FREQUENCY_MINUTES || '10', 10) * 60 * 1000)
}

run()
